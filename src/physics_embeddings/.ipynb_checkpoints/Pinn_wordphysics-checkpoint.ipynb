{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a9e2fd8e-7533-4814-b066-a07514a551bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "eba90b06-932a-4c26-b229-36b3d6236950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3ca0c16e-8a65-45ea-8ff4-52f847d75753",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # training\n",
    "    \"epochs\": 500,\n",
    "    \"batch_size\": 1,          # 1 utterance = 1 ODE\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"lambda_phys\": 1e-3,\n",
    "\n",
    "    # optimizer\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    # scheduler\n",
    "    \"use_scheduler\": True,\n",
    "    \"scheduler\": \"reduce_on_plateau\",\n",
    "    \"lr_patience\": 20,\n",
    "    \"lr_factor\": 0.5,\n",
    "    \"min_lr\": 1e-6,\n",
    "\n",
    "    # early stopping\n",
    "    \"early_stopping\": True,\n",
    "    \"early_patience\": 40,\n",
    "    \"min_delta\": 1e-6,\n",
    "\n",
    "    # model\n",
    "    \"embed_dim\": 48,\n",
    "\n",
    "    # physics\n",
    "    \"I\": 1.0,\n",
    "    \"r\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b76821-b318-472d-b8a5-e20d87e396dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "observed jaw kinematics : Iθ¨(t) + bθ˙(t) + kθ(t)  - torque = 0\n",
    "I = inertia, b= damping, k = stiffness\n",
    "learned articulatory embedding : z=fCNN​(θ,x,y)\n",
    "input = (x: [theta, w, a], t ), embeddings\n",
    "output = a(t) : force, (b,k,f_max)\n",
    "constant : I = jaw intertia\n",
    "            Moment arm = r\n",
    "torque model = τ = r.a(t).f_max (simplified)\n",
    "loss function: l_contrastive + l_kinetics\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5d60c6a2-e38e-43df-a84f-250652e64e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset ready\n",
    "#architecture\n",
    "#gradients\n",
    "#kinematics residual\n",
    "#contrastive residual \n",
    "#training loop\n",
    "# model evaluation \n",
    "#plotting residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "91e6f0f7-ad02-45bc-a3e2-6578bcf951b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 56])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('./../src/feature_extraction/z_word_embeddings.npz')\n",
    "\n",
    "word_ids = data[\"word_ids\"]       # (N_words,)\n",
    "embeddings = data[\"embeddings\"]   # (N_words, D)\n",
    "\n",
    "print(len(word_ids))\n",
    "# Word embeddings as a single tensor\n",
    "z_word_tensor = torch.tensor(\n",
    "    embeddings,\n",
    "    dtype=torch.float32\n",
    ")  # shape: (N_words, D)\n",
    "\n",
    "z_word_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "49739c9d-ae62-40c6-ae42-755b1ea39391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading & Normalizing PER SUBJECT...\n"
     ]
    }
   ],
   "source": [
    "#making data for the pinn model \n",
    "src_pattern = r'/workspace/Silent_Speech/dataset_sony/Normalized_dataset/recordings/*/*.csv'\n",
    "features = [\"theta\"]\n",
    "C = len(features)\n",
    "print(\"1. Loading & Normalizing PER SUBJECT...\")\n",
    "\n",
    "files = glob.glob(src_pattern)\n",
    "\n",
    "X, words, subjects = [], [], []\n",
    "T = 150\n",
    "\n",
    "for fp in files:\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        data = df[features].values\n",
    "        #normalization\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        \n",
    "        n_chunks = len(data) // T\n",
    "        if n_chunks == 0:\n",
    "            continue\n",
    "\n",
    "        data = data[:n_chunks * T].reshape(n_chunks, T, len(features))\n",
    "        X.append(data)\n",
    "\n",
    "        word = os.path.basename(os.path.dirname(fp))\n",
    "        subj = os.path.basename(fp).replace(\".csv\", \"\")\n",
    "\n",
    "        words.extend([word] * n_chunks)\n",
    "        subjects.extend([subj] * n_chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Skipping:\", fp, e)\n",
    "\n",
    "X = np.vstack(X)  # (N, T, 5)\n",
    "theta_all = X[..., 0] \n",
    "theta_all = torch.tensor(\n",
    "    theta_all,\n",
    "    dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2786d095-81ba-42ab-8d98-8f4939a01e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique words (strings)\n",
    "unique_words = sorted(list(set(words)))\n",
    "num_words = len(unique_words)\n",
    "\n",
    "# map word string → embedding row\n",
    "word_to_index = {w: i for i, w in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "72c0880d-b5fa-45e6-abe0-019323b90dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_word_tensor: (num_words, D)\n",
    "# must be in the SAME ORDER as unique_words\n",
    "\n",
    "z_word_tensor = torch.tensor(\n",
    "    embeddings,  # assuming embeddings are already ordered correctly\n",
    "    dtype=torch.float32\n",
    ")\n",
    "theta_all = theta_all.to(device)        # (N, T)\n",
    "z_word_tensor = z_word_tensor.to(device)  # (num_words, D)\n",
    "\n",
    "assert z_word_tensor.shape[0] == num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "af551ca5-9df4-4b1f-8e2f-17247099d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JawPINNDataset(Dataset):\n",
    "    def __init__(self, theta_all, words, z_word_tensor, word_to_index):\n",
    "        \"\"\"\n",
    "        theta_all     : (N, T)\n",
    "        words         : list[str], length N\n",
    "        z_word_tensor : (num_words, D)\n",
    "        word_to_index : dict[str -> int]\n",
    "        \"\"\"\n",
    "        self.theta_all = theta_all\n",
    "        self.words = words\n",
    "        self.z_word_tensor = z_word_tensor\n",
    "        self.word_to_index = word_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.theta_all.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        theta_obs = self.theta_all[idx]   # (T,)\n",
    "        word = self.words[idx]            # string\n",
    "        z_idx = self.word_to_index[word]  # int\n",
    "\n",
    "        return {\n",
    "            \"theta_obs\": theta_obs.unsqueeze(1),        # (T,1)\n",
    "            \"z_word\": self.z_word_tensor[z_idx]         # (D,)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "053d894c-c6ec-4b56-9383-ec9ff93aedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    JawPINNDataset(theta_all, words, z_word_tensor, word_to_index),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2c6727f8-bb6f-439b-a4f1-c38ed98eef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_tensor(batch_size, T, t0=0.0, t1=1.0, device=\"cuda\"):\n",
    "    t = torch.linspace(t0, t1, T, device=device)\n",
    "    t = t.view(1, T, 1).repeat(batch_size, 1, 1)\n",
    "    t.requires_grad_(True)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a3b83503-ca77-47aa-b509-0525b6e6025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "for batch in loader:\n",
    "    theta_obs = batch[\"theta_obs\"].to(device)  # (B, T, 1)\n",
    "    z_word = batch[\"z_word\"].to(device)        # (B, D)\n",
    "\n",
    "    B, T, _ = theta_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b937957f-5575-48e6-bc74-54ab64a85452",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = make_time_tensor(B, T)\n",
    "\n",
    "z_exp = z_word.unsqueeze(1).expand(B, T, -1)   # (B,T,D)\n",
    "pinn_input = torch.cat([t, z_exp], dim=-1)     # (B,T,1+D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0a3b0908-9a32-40cf-8719-ede8d966c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pinn_input.shape == (B, T, 1 + z_word.shape[1])\n",
    "assert theta_obs.is_cuda\n",
    "assert z_word.is_cuda\n",
    "assert t.is_cuda\n",
    "assert pinn_input.is_cuda\n",
    "assert t.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0783d971-245c-474f-9bd2-c52fd0d297cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ALL SHAPE + DEVICE CHECKS PASSED\n"
     ]
    }
   ],
   "source": [
    "D=56\n",
    "#shapes\n",
    "assert theta_obs.shape == (B, T, 1), theta_obs.shape\n",
    "assert z_word.shape == (B, D), z_word.shape\n",
    "assert t.shape == (B, T, 1), t.shape\n",
    "assert z_exp.shape == (B, T, D), z_exp.shape\n",
    "assert pinn_input.shape == (B, T, 1 + D), pinn_input.shape\n",
    "\n",
    "#device\n",
    "assert theta_obs.is_cuda\n",
    "assert z_word.is_cuda\n",
    "assert t.is_cuda\n",
    "assert pinn_input.is_cuda\n",
    "\n",
    "#grad flags\n",
    "assert t.requires_grad is True\n",
    "assert theta_obs.requires_grad is False\n",
    "assert z_word.requires_grad is False\n",
    "\n",
    "print(\"ALL SHAPE + DEVICE CHECKS PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd40e3-1417-4c80-85a1-904c3d3c1a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
