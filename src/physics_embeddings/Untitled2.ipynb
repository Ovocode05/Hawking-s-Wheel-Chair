{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d6eaa5-8169-4573-afdd-6f97341edb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c07d9b5-820f-4ee6-9123-e8fe135da31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading & Normalizing PER SUBJECT...\n",
      "(2177, 150, 3)\n",
      "['Dhadkan', 'Chah', 'Peshab', 'Dawai', 'Hospital', 'Bahaar', 'Paani', 'Peed', 'Khangh', 'Bukhar', 'Doctor', 'Dard', 'Jukham', 'Saah', 'Ulti', 'Bhuk', 'Neend', 'Khoon', 'Piyaas', 'Kabz', 'Ghabrahat', 'Kamjori', 'Gharde', 'Sardard', 'Chakkar']\n",
      "['madhav', 'KamalPreet', 'Surindar', 'Anupam', 'Bansbir', 'Armman', 'anubhavjot', 'Amish', 'harsh', 'Jaskaran']\n"
     ]
    }
   ],
   "source": [
    "src_pattern = r'/workspace/Krrish/Silent_Speech/dataset_sony/Normalized_dataset/recordings/*/*.csv'\n",
    "features = [\"theta\", \"x\", \"y\"]\n",
    "C = len(features)\n",
    "print(\"1. Loading & Normalizing PER SUBJECT...\")\n",
    "\n",
    "files = glob.glob(src_pattern)\n",
    "\n",
    "X, words, subjects = [], [], []\n",
    "T = 150\n",
    "\n",
    "for fp in files:\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        data = df[features].values\n",
    "        #normalization\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        \n",
    "        data = data[300:]   # remove calibration\n",
    "\n",
    "        if len(data) < 1500:\n",
    "            print(\"Warning: shorter than expected\", fp)\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(10):\n",
    "            start = i * 150\n",
    "            end = start + 150\n",
    "            segments.append(data[start:end])\n",
    "        \n",
    "        segments = np.stack(segments)\n",
    "\n",
    "        if n_chunks == 0:\n",
    "            continue\n",
    "\n",
    "        data = data[:n_chunks * T].reshape(n_chunks, T, len(features))\n",
    "        X.append(data)\n",
    "\n",
    "        word = os.path.basename(os.path.dirname(fp))\n",
    "        subj = os.path.basename(fp).replace(\".csv\", \"\")\n",
    "\n",
    "        words.extend([word] * n_chunks)\n",
    "        subjects.extend([subj] * n_chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Skipping:\", fp, e)\n",
    "\n",
    "X = np.vstack(X)  # (N, T, 5)\n",
    "print(X.shape)\n",
    "print(list(set(words)))\n",
    "print(list(set(subjects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bac3208-7beb-4b2d-9ef0-78d1f7132c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(words)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ea3bdd2-3b4e-4855-9198-0be83c287f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects: ['madhav', 'KamalPreet', 'Surindar', 'Anupam', 'Bansbir', 'Armman', 'anubhavjot', 'Amish', 'harsh', 'Jaskaran']\n",
      "Train shape: (1947, 150, 3)\n",
      "Test shape: (230, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_subjects = list(set(subjects))\n",
    "print(\"Subjects:\", unique_subjects)\n",
    "\n",
    "test_subject = unique_subjects[-1]  # pick one\n",
    "\n",
    "train_idx = [i for i, s in enumerate(subjects) if s != test_subject]\n",
    "test_idx  = [i for i, s in enumerate(subjects) if s == test_subject]\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c3145ed-f40e-4569-9fb7-c6afd8ad7435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transpose: (1947, 3, 150)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.transpose(X_train, (0, 2, 1))\n",
    "X_test  = np.transpose(X_test,  (0, 2, 1))\n",
    "\n",
    "print(\"After transpose:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "190bac80-210c-49a5-a5c1-7dd590597844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29b7d8b-41ec-40ae-8e51-b72cfda5c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JawCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(JawCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(3, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8aa3bea-5b44-4d81-b86b-a1eba81d8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JawCNN(num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e3acc46-81e8-4433-a935-10597d9e8537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 100.1847\n",
      "Epoch 5, Loss 96.3180\n",
      "Epoch 10, Loss 94.2479\n",
      "Epoch 15, Loss 92.7842\n",
      "Epoch 20, Loss 91.2477\n",
      "Epoch 25, Loss 89.8471\n",
      "Epoch 30, Loss 88.9557\n",
      "Epoch 35, Loss 87.6607\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    perm = torch.randperm(X_train_t.size(0))\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, X_train_t.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        xb = X_train_t[idx]\n",
    "        yb = y_train_t[idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0a0ff7c-4761-470c-bac4-caecfb3c92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 0.08695651590824127\n",
      "Top-2: 0.10869565217391304\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Top-1\n",
    "    top1 = torch.argmax(probs, dim=1)\n",
    "    top1_acc = (top1 == y_test_t).float().mean().item()\n",
    "\n",
    "    # Top-2\n",
    "    top2 = torch.topk(probs, k=2, dim=1).indices\n",
    "    correct_top2 = 0\n",
    "    for i in range(len(y_test_t)):\n",
    "        if y_test_t[i] in top2[i]:\n",
    "            correct_top2 += 1\n",
    "    top2_acc = correct_top2 / len(y_test_t)\n",
    "\n",
    "print(\"Top-1:\", top1_acc)\n",
    "print(\"Top-2:\", top2_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be833463-619c-4fb5-a3c2-72b7cf1a2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 2 0 0 1 0 0 6 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 3 0 0 0 0 0 4 0 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 3 0 0 4 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 0 1 0 0 2 0 0 0 1 0 4 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0 0 5 0 0 0 0 0 0 0 0 1 2 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 4 0 1 0 0 0 3 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 3 0 0 0 0 1 0 0 0 1 0 3 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 6 0 1 0 0 0 0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 3 0 0 0 2 2 1 0 0 1 0 0 0 0]\n",
      " [1 0 0 6 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 3 0 2 0 0 0 1 0 0 1 2 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0 0 2 0 0 0 0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 1 0 3 0 0 2 0 0 0 1 1 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 3 0 0 4 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 8 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 7 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    y_test_t.cpu().numpy(),\n",
    "    top1.cpu().numpy()\n",
    ")\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0578047-55db-487f-bde6-ec225bf19091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 0.08695651590824127\n",
      "Top-2: 0.10869565217391304\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Top-1\n",
    "    top1 = torch.argmax(probs, dim=1)\n",
    "    top1_acc = (top1 == y_test_t).float().mean().item()\n",
    "\n",
    "    # Top-2\n",
    "    top2 = torch.topk(probs, k=2, dim=1).indices\n",
    "    correct_top2 = sum(\n",
    "        [1 for i in range(len(y_test_t)) if y_test_t[i] in top2[i]]\n",
    "    )\n",
    "    top2_acc = correct_top2 / len(y_test_t)\n",
    "\n",
    "print(\"Top-1:\", top1_acc)\n",
    "print(\"Top-2:\", top2_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71f4a17a-db02-4c73-97eb-83c2f91e645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Dawai': 100, 'Dard': 100, 'Khoon': 100, 'Piyaas': 90, 'Doctor': 90, 'Hospital': 90, 'Peed': 90, 'Bukhar': 90, 'Paani': 90, 'Ulti': 90, 'Jukham': 89, 'Neend': 89, 'Kabz': 89, 'Chah': 89, 'Sardard': 89, 'Bhuk': 86, 'Kamjori': 80, 'Ghabrahat': 80, 'Bahaar': 80, 'Gharde': 80, 'Chakkar': 80, 'Peshab': 79, 'Saah': 79, 'Khangh': 79, 'Dhadkan': 79})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caa2d63a-a946-49bf-bfa9-947fdf638f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Counter({np.int64(6): 90, np.int64(5): 90, np.int64(16): 90, np.int64(21): 80, np.int64(8): 80, np.int64(11): 80, np.int64(19): 80, np.int64(2): 80, np.int64(18): 80, np.int64(24): 80, np.int64(12): 79, np.int64(17): 79, np.int64(22): 79, np.int64(15): 79, np.int64(13): 79, np.int64(3): 79, np.int64(23): 79, np.int64(1): 76, np.int64(14): 70, np.int64(9): 70, np.int64(0): 70, np.int64(10): 70, np.int64(4): 70, np.int64(20): 69, np.int64(7): 69})\n",
      "Test: Counter({np.int64(21): 10, np.int64(8): 10, np.int64(14): 10, np.int64(6): 10, np.int64(1): 10, np.int64(12): 10, np.int64(5): 10, np.int64(11): 10, np.int64(17): 10, np.int64(19): 10, np.int64(20): 10, np.int64(9): 10, np.int64(16): 10, np.int64(2): 10, np.int64(13): 10, np.int64(7): 10, np.int64(18): 10, np.int64(0): 10, np.int64(3): 10, np.int64(24): 10, np.int64(10): 10, np.int64(23): 10, np.int64(4): 10})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Train:\", Counter(y_train))\n",
    "print(\"Test:\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19f9d0fe-c156-4ffe-b455-116212907a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m T = \u001b[32m150\u001b[39m\n\u001b[32m      4\u001b[39m data_resampled = resample(data, T, axis=\u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m(data_resampled[np.newaxis, :, :])\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "from scipy.signal import resample\n",
    "\n",
    "T = 150\n",
    "data_resampled = resample(data, T, axis=0)\n",
    "X.append(data_resampled[np.newaxis, :, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b3eea4-4778-4c80-8b58-f2a894453152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Krrish/Silent_Speech/dataset_sony/Normalized_dataset/recordings/Piyaas/Amish.csv 1500\n",
      "/workspace/Krrish/Silent_Speech/dataset_sony/Normalized_dataset/recordings/Piyaas/Armman.csv 1500\n",
      "/workspace/Krrish/Silent_Speech/dataset_sony/Normalized_dataset/recordings/Piyaas/Bansbir.csv 1500\n",
      "/workspace/Krrish/Silent_Speech/dataset_sony/Normalized_dataset/recordings/Piyaas/Surindar.csv 1500\n",
      "/workspace/Krrish/Silent_Speech/dataset_sony/Normalized_dataset/recordings/Piyaas/KamalPreet.csv 1500\n"
     ]
    }
   ],
   "source": [
    "for fp in files[:5]:   # check a few first\n",
    "    df = pd.read_csv(fp)\n",
    "    total_len = len(df)\n",
    "    print(fp, total_len)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
