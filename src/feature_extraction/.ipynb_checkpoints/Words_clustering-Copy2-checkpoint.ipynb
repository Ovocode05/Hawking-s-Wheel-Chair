{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3fb76a2-08cf-43ae-b22c-0d9636a341d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f42f7675-c9dd-48ab-a448-c172dc3dd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f13239-376b-4d68-ac61-613b1abeb930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "223fff37-3a56-4287-9ffa-3bd9bcabcc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading & Normalizing PER SUBJECT...\n",
      "(5126, 150, 3)\n",
      "['Dawai', 'Chah', 'Doctor', 'Hospital', 'Ulti', 'Dhadkan', 'Khangh', 'Bukhar', 'Sardard', 'Dard', 'Chakkar', 'Bhuk', 'Saah', 'Kamjori', 'Jukham', 'Peed', 'Gharde', 'Kabz', 'Khoon', 'Ghabrahat', 'Paani', 'Bahaar', 'Neend', 'Piyaas', 'Peshab']\n",
      "['anubhavjot', 'Doctor', 'Hospital', 'Armman', 'madhav', 'Anupam', 'prabhdeep', 'liv', 'arushi', 'rudraksh', 'vansh', 'krrish', 'Krrish', 'mainder', 'suresh', 'anum', 'asha', 'gurmann', 'Amish', 'sapna', 'Bansbir', 'harsh', 'Jaskaran', 'KamalPreet', 'manj', 'Surindar', 'maninder', 'aunty']\n"
     ]
    }
   ],
   "source": [
    "src_pattern = r'/workspace/Krrish/Silent_Speech/dataset_sony/Normalized_dataset1/recordings/*/*.csv'\n",
    "features = [\"theta\", \"x\", \"y\"]\n",
    "C = len(features)\n",
    "print(\"1. Loading & Normalizing PER SUBJECT...\")\n",
    "\n",
    "files = glob.glob(src_pattern)\n",
    "\n",
    "X, words, subjects = [], [], []\n",
    "T = 150\n",
    "\n",
    "for fp in files:\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        data = df[features].values\n",
    "        #normalization\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        \n",
    "        n_chunks = len(data) // T\n",
    "        if n_chunks == 0:\n",
    "            continue\n",
    "\n",
    "        data = data[:n_chunks * T].reshape(n_chunks, T, len(features))\n",
    "        X.append(data)\n",
    "\n",
    "        word = os.path.basename(os.path.dirname(fp))\n",
    "        subj = os.path.basename(fp).replace(\".csv\", \"\")\n",
    "\n",
    "        words.extend([word] * n_chunks)\n",
    "        subjects.extend([subj] * n_chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Skipping:\", fp, e)\n",
    "\n",
    "X = np.vstack(X)  # (N, T, 5)\n",
    "print(X.shape)\n",
    "print(list(set(words)))\n",
    "print(list(set(subjects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341b2961-e6e9-4b49-a88a-72d1b8975f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Dataset & loaders ready.\n"
     ]
    }
   ],
   "source": [
    "#label coding \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(words)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32) # this is the data \n",
    "y_tensor = torch.tensor(y, dtype=torch.long) # this is the labels words to be predicted\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# full_loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=64,\n",
    "#     shuffle=False\n",
    "# )\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "full_loader = DataLoader( #it is important for batch and each batch contains 150 time frames\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"2. Dataset & loaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c7a7d1-1462-4d74-aca7-85f69f60e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinematicEncoder(nn.Module): \n",
    "    def __init__(self, in_ch=3, emb_dim=96):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1)  # removes timing/context\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(256, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        x = x.permute(0, 2, 1)  # (B, C, T)\n",
    "        h = self.net(x).squeeze(-1)\n",
    "        z = self.proj(h)\n",
    "        z = F.normalize(z, dim=1)  # critical for metric geometry\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f7f63ff-5447-4cb2-aabf-27d950841692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        features: (B, D), normalized\n",
    "        labels:   (B,)\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        B = features.shape[0]\n",
    "\n",
    "        sim = torch.matmul(features, features.T) / self.temperature\n",
    "        sim = sim - sim.max(dim=1, keepdim=True)[0]  # numerical stability\n",
    "\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(B, device=device)\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        exp_sim = torch.exp(sim) * logits_mask\n",
    "        log_prob = sim - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n",
    "        loss = -mean_log_prob_pos.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078aab5f-7e27-4616-923e-6731cd43b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_jitter_torch(x, max_shift=20):\n",
    "    # x: (T, C)\n",
    "    shift = torch.randint(-max_shift, max_shift + 1, (1,)).item()\n",
    "    return torch.roll(x, shifts=shift, dims=0)\n",
    "\n",
    "def channel_dropout_torch(x, p=0.2):\n",
    "    # x: (T, C)\n",
    "    mask = (torch.rand(x.shape[1]) > p).float()\n",
    "    return x * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bc67283-ae03-4a09-afbc-bd211519b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 3.7960\n",
      "Epoch 20 | Loss: 3.8123\n",
      "Epoch 30 | Loss: 3.7766\n",
      "Epoch 40 | Loss: 3.7783\n",
      "Epoch 50 | Loss: 3.7591\n",
      "Epoch 60 | Loss: 3.7550\n",
      "Epoch 70 | Loss: 3.7100\n",
      "Epoch 80 | Loss: 3.7069\n",
      "Epoch 90 | Loss: 3.7043\n",
      "Epoch 100 | Loss: 3.6951\n",
      "Epoch 110 | Loss: 3.6839\n",
      "Epoch 120 | Loss: 3.6687\n",
      "Epoch 130 | Loss: 3.6491\n",
      "Epoch 140 | Loss: 3.6627\n",
      "Epoch 150 | Loss: 3.6395\n",
      "Epoch 160 | Loss: 3.6253\n",
      "Epoch 170 | Loss: 3.5987\n",
      "Epoch 180 | Loss: 3.6010\n",
      "Epoch 190 | Loss: 3.6168\n",
      "Epoch 200 | Loss: 3.5499\n",
      "Epoch 210 | Loss: 3.5799\n",
      "Epoch 220 | Loss: 3.5405\n",
      "Epoch 230 | Loss: 3.5283\n",
      "Epoch 240 | Loss: 3.5121\n",
      "Epoch 250 | Loss: 3.5244\n",
      "Epoch 260 | Loss: 3.5090\n",
      "Epoch 270 | Loss: 3.5003\n",
      "Epoch 280 | Loss: 3.4434\n",
      "Epoch 290 | Loss: 3.4532\n",
      "Epoch 300 | Loss: 3.4915\n",
      "Epoch 310 | Loss: 3.3975\n",
      "Epoch 320 | Loss: 3.4108\n",
      "Epoch 330 | Loss: 3.4176\n",
      "Epoch 340 | Loss: 3.3955\n",
      "Epoch 350 | Loss: 3.3786\n",
      "Epoch 360 | Loss: 3.3554\n",
      "Epoch 370 | Loss: 3.3258\n",
      "Epoch 380 | Loss: 3.3174\n",
      "Epoch 390 | Loss: 3.3265\n",
      "Epoch 400 | Loss: 3.3189\n",
      "Epoch 410 | Loss: 3.2943\n",
      "Epoch 420 | Loss: 3.2374\n",
      "Epoch 430 | Loss: 3.2314\n",
      "Epoch 440 | Loss: 3.2267\n",
      "Epoch 450 | Loss: 3.2468\n",
      "Epoch 460 | Loss: 3.2112\n",
      "Epoch 470 | Loss: 3.1927\n",
      "Epoch 480 | Loss: 3.1708\n",
      "Epoch 490 | Loss: 3.1483\n",
      "Epoch 500 | Loss: 3.1377\n"
     ]
    }
   ],
   "source": [
    "model = KinematicEncoder(in_ch=C, emb_dim=64).to(device)\n",
    "criterion = SupConLoss(temperature=0.06)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=4e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer,\n",
    "#     mode=\"min\",\n",
    "#     factor=0.5,      # reduce LR by half\n",
    "#     patience=15,     # wait 15 epochs without improvement\n",
    "#     min_lr=1e-5,\n",
    "# )\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # keep everything in torch\n",
    "        x_batch = x_batch.clone()   # VERY IMPORTANT\n",
    "\n",
    "        for i in range(x_batch.shape[0]):\n",
    "            x_batch[i] = temporal_jitter_torch(x_batch[i])\n",
    "            x_batch[i] = channel_dropout_torch(x_batch[i])\n",
    "\n",
    "        x = x_batch.to(device)\n",
    "        y = y_batch.to(device)\n",
    "\n",
    "        z = model(x)\n",
    "        loss = criterion(z, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71e603f8-7153-4c7f-9180-a84a21bc2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "model.eval()\n",
    "\n",
    "chunk_embeddings = []\n",
    "chunk_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in full_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "\n",
    "        z = model(x_batch)              # (B, D), already normalized\n",
    "        z = z.cpu().numpy()\n",
    "\n",
    "        chunk_embeddings.append(z)\n",
    "        chunk_labels.append(y_batch.numpy())\n",
    "\n",
    "chunk_embeddings = np.vstack(chunk_embeddings)   # (N_chunks, D)\n",
    "chunk_labels = np.concatenate(chunk_labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9774fce5-b9b1-4083-b36a-bffb6b0f6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_embs = defaultdict(list)\n",
    "\n",
    "for z, word_id in zip(chunk_embeddings, chunk_labels):\n",
    "    word_to_embs[word_id].append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de843e00-b0cb-4cfd-a078-b1d9b1c207de",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_word = {}\n",
    "\n",
    "for word_id, embs in word_to_embs.items():\n",
    "    embs = np.stack(embs, axis=0)   # (N_chunks_word, D)\n",
    "\n",
    "    mean_emb = embs.mean(axis=0)\n",
    "\n",
    "    # CRITICAL: renormalize (stay on unit hypersphere)\n",
    "    mean_emb = mean_emb / np.linalg.norm(mean_emb)\n",
    "\n",
    "    z_word[word_id] = mean_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65bb19ef-5abc-4b20-b908-9e8083acdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_word_torch = {\n",
    "    word_id: torch.tensor(vec, dtype=torch.float32)\n",
    "    for word_id, vec in z_word.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6b92d5f-17ee-41d3-ac0f-f1c5234a8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_z_word(word_id, T):\n",
    "    \"\"\"\n",
    "    Returns z_word repeated across time\n",
    "    \"\"\"\n",
    "    z = z_word_torch[word_id]        # (D,)\n",
    "    z = z.unsqueeze(0).repeat(T, 1)  # (T, D)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "813916b4-189a-4508-a7a4-f4af0ece2a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inter-word similarity: 0.8189812\n"
     ]
    }
   ],
   "source": [
    "# 1. Norm check\n",
    "for w, z in z_word.items():\n",
    "    assert np.isclose(np.linalg.norm(z), 1.0, atol=1e-3)\n",
    "\n",
    "# 2. Distinctness check\n",
    "keys = list(z_word.keys())\n",
    "sims = []\n",
    "for i in range(len(keys)):\n",
    "    for j in range(i + 1, len(keys)):\n",
    "        sim = np.dot(z_word[keys[i]], z_word[keys[j]])\n",
    "        sims.append(sim)\n",
    "\n",
    "print(\"Mean inter-word similarity:\", np.mean(sims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fa3e4d7-1090-48ee-b738-62fa3b2c51d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 25\n",
      "Embedding dim: (64,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Words:\", len(z_word))\n",
    "print(\"Embedding dim:\", next(iter(z_word.values())).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06bc1f46-521c-4d9c-b6a4-5fc2938ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = np.array(list(z_word.keys()))             # (N_words,)\n",
    "embeddings = np.stack(list(z_word.values()), axis=0) # (N_words, D)\n",
    "\n",
    "np.savez(\n",
    "    \"z_word_embeddings.npz\",\n",
    "    word_ids=word_ids,\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6444363-1c86-45e9-a73d-a0a9a0a1f0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
