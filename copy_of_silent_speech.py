# -*- coding: utf-8 -*-
"""Copy of Silent_Speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JWGXiTYLe2SgRCl4GjhFPMzr3H6mmWRF
"""

# List of all 25 words (dataset utterances)
WORDS = [
    "Bahaar", "Bhuk", "Bukhar", "Chah", "Chakkar", "Dard", "Dawai", "Dhadkan",
    "Doctor", "Ghabrahat", "Gharde", "Hospital", "Jukham", "Kabz", "Kamjori",
    "Khangh", "Khoon", "Neend", "Panni", "Peed", "Peshab", "Piyaas", "Saah",
    "Sardard", "Ulti"
]

# List of all 8 subjects (recorded speakers)
SUBJECTS = [
    "Amish", "anubhavjot", "Armman", "Bansbir",
    "Jaskaran", "KamalPreet", "madhav", "Surindar"
]

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#loading the data ans making a data frame
df = pd.read_csv('Bansbir.csv')
df.head()

#keep the top 1800 values
df_new = df.head(1800)

#calculate sampling rate
total_frames = len(df_new)
duration = 60
sampling_rate = total_frames / duration
print(sampling_rate)

#segmenting 60s into 5s interval
frames_per_interval = int(round(5 * sampling_rate))
print(frames_per_interval)
n_segments = total_frames // frames_per_interval
print(n_segments)
segments = []
for i in range(n_segments):
  start = i * frames_per_interval
  end = start + frames_per_interval
  segments.append(df.iloc[start:end].copy().reset_index(drop=True))

#dividing the sengments as per role:
#first 2 in as minima and maximum
#remaining as the word utterance
[ref_min, ref_max] = segments[:2]
utterance = segments[2:]

#finding the reference angle (minima)
ref_min = ref_min.fillna(0.0)
reference_angle = ref_min[ref_min["theta"]>0]["theta"].mean()
reference_angle

reference_max_angle_avg = ref_max[ref_max["theta"]>reference_angle]["theta"].mean()
reference_max_angle = ref_max["theta"].max()
reference_max_angle_avg = reference_max_angle_avg if pd.notnull(reference_max_angle_avg) else 0.0
reference_max_angle = reference_max_angle if pd.notnull(reference_max_angle) else 0.0
reference_max_angle_avg, reference_max_angle

"""Filling out nan values using Decision tree regression"""

from sklearn.impute import KNNImputer # Import KNNImputer

for i, segment_df in enumerate(utterance):
    print(f"Processing segment {i+1}/{len(utterance)}...")

    # Check if any NaN values exist in the current segment DataFrame
    if not segment_df.isnull().any().any():
        print(f"  No NaN values found in segment {i+1}. Skipping imputation.")
        continue

    print(f"  NaN values found in segment {i+1}. Applying KNN Imputer...")

    # Initialize KNNImputer with n_neighbors=5 as requested
    imputer = KNNImputer(n_neighbors=5)

    # Apply KNNImputer to the entire DataFrame.
    # KNNImputer returns a numpy array, so we convert it back to a DataFrame.
    imputed_data = imputer.fit_transform(segment_df.copy())

    # Create a new DataFrame from the imputed data, retaining original column names and index
    imputed_df = pd.DataFrame(imputed_data, columns=segment_df.columns, index=segment_df.index)

    # Update the corresponding segment in the 'utterance' list with the imputed DataFrame
    utterance[i] = imputed_df

print("NaN filling complete with KNN Imputer for all utterance segments.")

all_nan_filled = True
for i, segment_df in enumerate(utterance):
    if segment_df.isnull().any().any():
        print(f"Segment {i+1} still contains NaN values.")
        all_nan_filled = False
        break

if all_nan_filled:
    print("All NaN values in all utterance segments have been successfully filled.")
else:
    print("Some NaN values still exist in utterance segments.")

import matplotlib.pyplot as plt

for i, segment_df in enumerate(utterance):
    plt.figure(figsize=(10, 5))
    plt.plot(segment_df.index, segment_df['theta'])
    plt.title(f'Theta values for Utterance Segment {i+1}')
    plt.xlabel('Frame Index (within segment)')
    plt.ylabel('Theta')
    plt.grid(True)
    plt.show()

import numpy as np

for i, segment_df in enumerate(utterance):
    # Identify spikes: values below reference_angle or above reference_max_angle
    spikes_mask = (segment_df['theta'] < reference_angle) | (segment_df['theta'] > reference_max_angle)

    # Replace spikes with NaN
    utterance[i].loc[spikes_mask, 'theta'] = np.nan

    # Fill NaN values using linear interpolation
    # limit_direction='both' ensures NaNs at the beginning/end are filled if possible
    # limit_area='inside' interpolates only within existing non-NaN values
    utterance[i]['theta'] = utterance[i]['theta'].interpolate(method='linear', limit_direction='both', limit_area='inside')

print("Spikes removed and NaNs filled with linear interpolation for all utterance segments.")

for i, segment_df in enumerate(utterance):
    # Normalize 'theta' values to a 0-1 scale
    utterance[i]['theta'] = (segment_df['theta'] - segment_df['theta'].mean()) / (segment_df['theta'].std())

print("Theta values normalized to 0-1 scale for all utterance segments.")

import matplotlib.pyplot as plt

for i, segment_df in enumerate(utterance):
    plt.figure(figsize=(10, 5))
    plt.plot(segment_df["frame_idx"], segment_df['theta'], label=f'Normalized Theta (Segment {i+1})')
    plt.title(f'Normalized Theta values for Utterance Segment {i+1}')
    plt.xlabel('Frame Index (within segment)')
    plt.ylabel('Normalized Theta (0-1 Scale)')
    plt.grid(True)
    plt.legend()
    plt.show()

print("Visualizations of normalized theta values for all utterance segments completed.")

#problem is that for subjects that have min and max angle ~ 1-2 degrees than, normalization is tough.

all_utterance_features = []

# Function to calculate Zero Crossing Rate
def calculate_zcr(signal):
    """Calculates the Zero Crossing Rate of a 1D signal."""
    # Ensure signal is numeric and handle potential NaNs (though they should be filled by now)
    signal = signal.dropna()
    if signal.empty:
        return 0.0
    # Calculate zero-crossing rate: count sign changes / total samples
    return np.sum(np.abs(np.diff(np.sign(signal)))) / (2 * len(signal))


for i, segment_df in enumerate(utterance):
    segment_features = {'segment_id': i + 1}

    # Define relevant numeric columns for statistical features
    # 'theta' has been normalized, 'x', 'y' are coordinates, 'omega', 'alpha' are velocities/accelerations
    numeric_cols = ['theta', 'x', 'y', 'omega', 'alpha']

    for col in numeric_cols:
        if col in segment_df.columns:
            # Calculate Skewness
            segment_features[f'{col}_skewness'] = segment_df[col].skew()

            # Calculate Kurtosis
            segment_features[f'{col}_kurtosis'] = segment_df[col].kurt()

            # Calculate Energy (sum of squares)
            segment_features[f'{col}_energy'] = np.sum(segment_df[col]**2)
        else:
            print(f"Warning: Column '{col}' not found in segment {i+1}. Skipping skewness, kurtosis, energy.")

    # Calculate Zero Crossing Rate for 'omega' and 'alpha'
    if 'omega' in segment_df.columns:
        segment_features['omega_zcr'] = calculate_zcr(segment_df['omega'])
    else:
        print(f"Warning: 'omega' column not found in segment {i+1}. Skipping omega ZCR.")

    if 'alpha' in segment_df.columns:
        segment_features['alpha_zcr'] = calculate_zcr(segment_df['alpha'])
    else:
        print(f"Warning: 'alpha' column not found in segment {i+1}. Skipping alpha ZCR.")

    all_utterance_features.append(segment_features)

# Convert the list of dictionaries to a DataFrame
features_df = pd.DataFrame(all_utterance_features)

print("Statistical Features for Utterance Segments:")
print(features_df)

all_utterance_features = []
numeric_cols = ['theta', 'x', 'y', 'omega', 'alpha']

for i, segment_df in enumerate(utterance):
    segment_features = {'segment_id': i + 1}

    for col in numeric_cols:
        if col in segment_df.columns:
            # Calculate skewness
            segment_features[f'{col}_skewness'] = segment_df[col].skew()
            # Calculate kurtosis
            segment_features[f'{col}_kurtosis'] = segment_df[col].kurt()

    all_utterance_features.append(segment_features)

print("Calculated skewness and kurtosis for all relevant numeric columns in each utterance segment.")
# print the first few feature sets to verify
print("\nExample of extracted features for the first two segments:")
print(all_utterance_features[:2])

all_utterance_features_updated = []
numeric_cols = ['theta', 'x', 'y', 'omega', 'alpha']

for i, segment_df in enumerate(utterance):
    # Initialize segment_features with segment_id
    segment_features = {'segment_id': i + 1}

    for col in numeric_cols:
        if col in segment_df.columns:
            # Calculate skewness and kurtosis (re-adding these as per the full logic)
            segment_features[f'{col}_skewness'] = segment_df[col].skew()
            segment_features[f'{col}_kurtosis'] = segment_df[col].kurt()

            # Calculate Energy (sum of squares)
            segment_features[f'{col}_energy'] = np.sum(segment_df[col]**2)
        else:
            print(f"Warning: Column '{col}' not found in segment {i+1}. Skipping skewness, kurtosis, energy.")

    all_utterance_features_updated.append(segment_features)

# Update the main feature list
all_utterance_features = all_utterance_features_updated

print("Calculated energy for all relevant numeric columns in each utterance segment.")
# print the first few feature sets to verify
print("\nExample of extracted features for the first two segments with energy:")
print(all_utterance_features[:2])

all_utterance_features = []
numeric_cols = ['theta', 'x', 'y', 'omega', 'alpha']

def calculate_zcr(signal):
    """Calculates the Zero Crossing Rate of a 1D signal."""
    signal = signal.dropna()
    if signal.empty:
        return 0.0
    return np.sum(np.abs(np.diff(np.sign(signal)))) / (2 * len(signal))

for i, segment_df in enumerate(utterance):
    segment_features = {'segment_id': i + 1}

    for col in numeric_cols:
        if col in segment_df.columns:
            # Calculate skewness
            segment_features[f'{col}_skewness'] = segment_df[col].skew()
            # Calculate kurtosis
            segment_features[f'{col}_kurtosis'] = segment_df[col].kurt()
            # Calculate Energy (sum of squares)
            segment_features[f'{col}_energy'] = np.sum(segment_df[col]**2)
        else:
            print(f"Warning: Column '{col}' not found in segment {i+1}. Skipping skewness, kurtosis, energy.")

    # Calculate Zero Crossing Rate for 'omega' and 'alpha'
    if 'omega' in segment_df.columns:
        segment_features['omega_zcr'] = calculate_zcr(segment_df['omega'])
    else:
        print(f"Warning: 'omega' column not found in segment {i+1}. Skipping omega ZCR.")

    if 'alpha' in segment_df.columns:
        segment_features['alpha_zcr'] = calculate_zcr(segment_df['alpha'])
    else:
        print(f"Warning: 'alpha' column not found in segment {i+1}. Skipping alpha ZCR.")

    all_utterance_features.append(segment_features)

print("Zero-crossing rates calculated and added to features for all utterance segments.")
# print the first few feature sets to verify
print("\nExample of extracted features for the first two segments with ZCR:")
print(all_utterance_features[:2])

features_df = pd.DataFrame(all_utterance_features)

print("Consolidated Statistical Features for Utterance Segments:")
print(features_df)

import matplotlib.pyplot as plt

for feat in ['theta_energy', 'theta_skewness', 'theta_kurtosis']:
    plt.plot(features_df['segment_id'], features_df[feat], marker='o', label=feat)

plt.title("Feature evolution across segments for 'bahaar'")
plt.xlabel("Segment index (time order)")
plt.ylabel("Feature value")
plt.legend()
plt.show()

import numpy as np
from scipy.stats import entropy

def extract_spectral_features(signal, sampling_rate):
    """Calculates spectral features for a given signal."""
    signal_clean = signal.dropna().to_numpy() if hasattr(signal, 'dropna') else np.asarray(signal)

    # 3a. Handle edge cases
    if len(signal_clean) < 2 or np.all(signal_clean == 0):
        return {
            'dominant_frequency': np.nan,
            'spectral_centroid': np.nan,
            'spectral_bandwidth': np.nan,
            'spectral_entropy': np.nan,
            'power_spectral_density': np.nan
        }

    n = len(signal_clean)
    yf = np.fft.fft(signal_clean)
    xf = np.fft.fftfreq(n, 1 / sampling_rate)

    # 5. Extract magnitudes of positive frequency components and corresponding frequencies
    idx_pos = np.where(xf >= 0)
    xf_pos = xf[idx_pos]
    yf_pos_mag = np.abs(yf[idx_pos])

    # Avoid division by zero or log of zero for empty or all-zero magnitudes
    if np.sum(yf_pos_mag) == 0:
        return {
            'dominant_frequency': np.nan,
            'spectral_centroid': np.nan,
            'spectral_bandwidth': np.nan,
            'spectral_entropy': np.nan,
            'power_spectral_density': np.nan
        }

    # 6a. Dominant Frequency
    dominant_frequency = xf_pos[np.argmax(yf_pos_mag)]

    # 6b. Spectral Centroid
    spectral_centroid = np.sum(xf_pos * yf_pos_mag) / np.sum(yf_pos_mag)

    # 6c. Spectral Bandwidth
    spectral_bandwidth = np.sqrt(np.sum(((xf_pos - spectral_centroid)**2) * yf_pos_mag) / np.sum(yf_pos_mag))

    # 6d. Spectral Entropy
    # Normalize magnitudes to form a probability distribution
    magnitudes_normalized = yf_pos_mag / np.sum(yf_pos_mag)
    spectral_entropy = entropy(magnitudes_normalized + np.finfo(float).eps) # Add epsilon to avoid log(0)

    # 6e. Power Spectral Density (PSD)
    psd = (yf_pos_mag**2) / n # For one-sided spectrum, usually scale by 2/N, but for definition using magnitudes, this is fine
    power_spectral_density = np.sum(psd)

    return {
        'dominant_frequency': dominant_frequency,
        'spectral_centroid': spectral_centroid,
        'spectral_bandwidth': spectral_bandwidth,
        'spectral_entropy': spectral_entropy,
        'power_spectral_density': power_spectral_density
    }

print("Defined 'extract_spectral_features' function.")

for i, segment_df in enumerate(utterance):
    # Extract theta signal for the current segment
    theta_signal = segment_df['theta']

    # Apply the spectral feature extraction function
    spectral_features = extract_spectral_features(theta_signal, sampling_rate)

    # Update the existing feature dictionary for this segment
    # The all_utterance_features list should already be initialized with other features
    # Find the dictionary for the current segment_id and update it
    current_segment_id = i + 1
    for segment_feature_dict in all_utterance_features:
        if segment_feature_dict['segment_id'] == current_segment_id:
            segment_feature_dict.update(spectral_features)
            break

print("Spectral features extracted and added to all utterance segments.")

# print the first few feature sets to verify
print("\nExample of extracted features for the first two segments with spectral features:")
print(all_utterance_features[:2])

features_df = pd.DataFrame(all_utterance_features)

print("Consolidated Statistical and Spectral Features for Utterance Segments:")
print(features_df.head())

spectral_features_df = features_df[['segment_id', 'dominant_frequency', 'spectral_centroid', 'spectral_bandwidth', 'spectral_entropy', 'power_spectral_density']]

print("Frequency-Dependent Features DataFrame:")
print(spectral_features_df)

"""## Final Task

### Subtask:
Review the consolidated DataFrame of frequency-domain features to identify significant patterns or changes in jaw oscillation characteristics across utterance segments.

## Summary:

### Data Analysis Key Findings

*   A Python function, `extract_spectral_features`, was successfully implemented to compute five spectral features (dominant frequency, spectral centroid, spectral bandwidth, spectral entropy, and power spectral density) from a time series and its sampling rate. This function includes robust handling for edge cases such as short signals or all-zero signals.
*   The `extract_spectral_features` function was applied to the 'theta' column of each utterance segment, and the computed features were successfully integrated into the existing feature dictionaries for each segment.
*   All extracted features, including both statistical and the newly computed spectral features, were consolidated into a single pandas DataFrame named `features_df`. This DataFrame contains 23 columns, encompassing `segment_id`, various statistical features, and the five new spectral features.

### Insights or Next Steps

*   The consolidated `features_df` is now ready for detailed exploratory data analysis to identify significant patterns, correlations, or changes in jaw oscillation characteristics across different utterance segments, as per the original subtask objective.
*   Further analysis could involve statistical tests or visualization techniques to compare spectral features across different segments or conditions, potentially revealing insights into speech production mechanics.
"""

import matplotlib.pyplot as plt

# List of spectral features to plot
spectral_feature_cols = [
    'dominant_frequency',
    'spectral_centroid',
    'spectral_bandwidth',
    'spectral_entropy',
    'power_spectral_density'
]

# Create a figure and a set of subplots
fig, axes = plt.subplots(len(spectral_feature_cols), 1, figsize=(12, 4 * len(spectral_feature_cols)), sharex=True)

# Plot each spectral feature
for i, feat in enumerate(spectral_feature_cols):
    axes[i].plot(spectral_features_df['segment_id'], spectral_features_df[feat], marker='o', linestyle='-')
    axes[i].set_title(f'Temporal Evolution of {feat.replace("_", " ").title()}')
    axes[i].set_ylabel(feat.replace("_", " ").title())
    axes[i].grid(True)

axes[-1].set_xlabel('Segment ID') # Set x-label only for the last subplot
plt.tight_layout()
plt.show()

print("Visualizations of frequency-domain features completed.")



"""# Task
Implement a Python function to calculate the Approximate Entropy (ApEn) for a given time series, ensuring it accepts parameters for embedding dimension (m) and tolerance (r) and includes robust handling for edge cases like short or constant signals.

## Implement Approximate Entropy Function

### Subtask:
Create a Python function to calculate the Approximate Entropy (ApEn) for a given time series, ensuring it accepts parameters for embedding dimension (m) and tolerance (r) and includes robust handling for edge cases like short or constant signals.

**Reasoning**:
The user wants a Python function to calculate Approximate Entropy (ApEn). I need to define the `approx_entropy` function with the specified parameters, implement robust edge case handling for short or constant signals, and then proceed with the core ApEn algorithm steps including embedding vectors and calculating matching pairs to determine phi_m and phi_m_plus_1.
"""

import numpy as np

def approx_entropy(signal, m, r):
    """Calculates the Approximate Entropy (ApEn) for a given time series.

    Args:
        signal (array_like): The time series data.
        m (int): The embedding dimension.
        r (float): The tolerance level (usually a proportion of the standard deviation of the signal).

    Returns:
        float: The Approximate Entropy value, or np.nan/0.0 for edge cases.
    """
    signal_arr = np.asarray(signal).flatten()
    signal_arr = signal_arr[~np.isnan(signal_arr)] # Remove NaN values

    N = len(signal_arr)

    # 3. Implement robust edge case handling:
    # Check if the length of the signal is too short
    if N < m + 2:
        print(f"Warning: Signal length {N} is too short for m={m}. Returning NaN.")
        return np.nan

    # Check if the signal is constant
    if np.std(signal_arr) == 0:
        print("Warning: Signal is constant. Returning 0.0 (complexity is zero).")
        return 0.0

    # Scale tolerance 'r' by the standard deviation of the signal
    # This is a common practice for ApEn to make 'r' scale-independent
    r = r * np.std(signal_arr)

    # 4. Implement the core Approximate Entropy algorithm:

    # Helper function to create embedded vectors
    def _embed_vectors(data, dim):
        return np.array([data[i:i + dim] for i in range(N - dim + 1)])

    # Helper function to count matching pairs
    def _count_matches(embedded_vectors, dim):
        count = np.zeros(N - dim + 1)
        for i in range(N - dim + 1):
            # Calculate Chebyshev distance (max absolute difference)
            dist = np.max(np.abs(embedded_vectors - embedded_vectors[i]), axis=1)
            count[i] = np.sum(dist <= r) # Count pairs within tolerance 'r'
        # Return normalized count: sum of log probabilities
        return np.sum(np.log(count / (N - dim + 1)))

    # Calculate phi_m and phi_m_plus_1
    embedded_m = _embed_vectors(signal_arr, m)
    phi_m = _count_matches(embedded_m, m)

    embedded_m_plus_1 = _embed_vectors(signal_arr, m + 1)
    phi_m_plus_1 = _count_matches(embedded_m_plus_1, m + 1)

    # Compute Approximate Entropy
    ap_en = (phi_m - phi_m_plus_1)

    return ap_en

print("Defined 'approx_entropy' function.")

"""## Implement Sample Entropy Function

### Subtask:
Create a Python function to calculate the Sample Entropy (SampEn) for a given time series. Similar to Approximate Entropy, it should accept embedding dimension (m) and tolerance (r) as parameters, with robust handling for edge cases.

**Reasoning**:
I need to define the `sample_entropy` function as per the instructions, including input validation, edge case handling, helper functions for embedding vectors and counting matches, and then calculate Sample Entropy using the specified formula.
"""

import numpy as np

def sample_entropy(signal, m, r):
    """Calculates the Sample Entropy (SampEn) for a given time series.

    Args:
        signal (array_like): The time series data.
        m (int): The embedding dimension.
        r (float): The tolerance level (usually a proportion of the standard deviation of the signal).

    Returns:
        float: The Sample Entropy value, or np.nan/0.0 for edge cases.
    """
    signal_arr = np.asarray(signal).flatten()
    signal_arr = signal_arr[~np.isnan(signal_arr)]  # Remove NaN values

    N = len(signal_arr)

    # 3. Implement robust edge case handling:
    # Check if the length of the signal is too short
    if N < m + 2:
        print(f"Warning: Signal length {N} is too short for m={m}. Returning NaN.")
        return np.nan

    # Check if the signal is constant
    if np.std(signal_arr) == 0:
        print("Warning: Signal is constant. Returning 0.0 (complexity is zero).")
        return 0.0

    # Scale tolerance 'r' by the standard deviation of the signal
    # This is a common practice for SampEn to make 'r' scale-independent
    r = r * np.std(signal_arr)

    # 5. Helper function to create embedded vectors
    def _embed_vectors(data, dim):
        # Return a 2D array where each row is an embedded vector
        return np.array([data[i:i + dim] for i in range(N - dim + 1)])

    # 6. Helper function to count matching pairs (excluding self-matches)
    def _count_matches(embedded_vectors, dim):
        # N_dim represents the number of embedded vectors
        N_dim = N - dim + 1

        # Initialize count for each vector
        counts = np.zeros(N_dim)

        for i in range(N_dim):
            # Calculate Chebyshev distance (max absolute difference)
            # between current vector and all other vectors
            dist = np.max(np.abs(embedded_vectors - embedded_vectors[i]), axis=1)

            # Count matches, excluding self-matches (dist[i] is always 0)
            # The condition `dist <= r` will include self-matches, so we subtract 1 later
            counts[i] = np.sum(dist <= r) - 1 # Subtract 1 for self-match

        # Sum of counts, which is essentially the total number of matching pairs (A_m or B_m)
        return np.sum(counts)

    # 7. Calculate C_m (or B_m in some notations) for embedding dimension m
    embedded_m = _embed_vectors(signal_arr, m)
    C_m = _count_matches(embedded_m, m)

    # 8. Calculate C_m_plus_1 (or A_m in some notations) for embedding dimension m + 1
    embedded_m_plus_1 = _embed_vectors(signal_arr, m + 1)
    C_m_plus_1 = _count_matches(embedded_m_plus_1, m + 1)

    # Handle case where no matches are found, to prevent log(0)
    if C_m_plus_1 == 0 or C_m == 0:
        return np.nan

    # 9. Compute Sample Entropy
    samp_en = -np.log(C_m_plus_1 / C_m)

    return samp_en

print("Defined 'sample_entropy' function.")

"""## Implement Correlation Dimension Function

### Subtask:
Create a Python function to estimate the Correlation Dimension for a given time series. This function will require parameters for the embedding dimension and a radius (epsilon), and should be designed to handle various data conditions.

**Reasoning**:
I will define the `correlation_dimension` function following the instructions, including signal preprocessing, edge case handling, embedding vector creation, distance calculation, and correlation sum computation.
"""

import numpy as np
from scipy.spatial.distance import pdist, squareform

def correlation_dimension(signal, m, epsilon):
    """Estimates the Correlation Dimension for a given time series.

    Args:
        signal (array_like): The time series data.
        m (int): The embedding dimension.
        epsilon (float): The radius for counting neighbors.

    Returns:
        float: The logarithm of the correlation sum (log(C(epsilon))), or np.nan/0.0 for edge cases.
    """
    signal_arr = np.asarray(signal).flatten()
    signal_arr = signal_arr[~np.isnan(signal_arr)]  # Remove NaN values

    N = len(signal_arr)

    # 3. Implement robust edge case handling:
    # a. Check if the length of the signal is too short
    if N < m:
        print(f"Warning: Signal length {N} is too short for embedding dimension m={m}. Returning NaN.")
        return np.nan

    # b. Check if the signal is constant
    if np.std(signal_arr) == 0:
        print("Warning: Signal is constant. Returning 0.0 (correlation dimension is zero).")
        return 0.0

    # 4. Create embedded vectors (delay embeddings)
    embedded_vectors = np.array([signal_arr[i:i + m] for i in range(N - m + 1)])
    N_m = embedded_vectors.shape[0]

    # 5. Calculate the pairwise Euclidean distances between all embedded vectors.
    # Using pdist for efficiency, then squareform to get a square matrix.
    distances = pdist(embedded_vectors, 'euclidean')
    distance_matrix = squareform(distances)

    # 6. For each embedded vector, count how many other embedded vectors are within the epsilon radius (excluding self-matches).
    # We create a boolean matrix where True means distance <= epsilon
    # and then exclude the diagonal (self-matches).
    # np.fill_diagonal(distance_matrix, np.inf) could also work, but direct counting is clear.

    # Count pairs where distance is less than or equal to epsilon, excluding self-pairs
    # For each row, count how many elements are <= epsilon (excluding the diagonal)
    # The total count is the sum of these, and since distance_matrix is symmetric, we count each pair twice (i,j) and (j,i)
    # So, we sum the upper triangle (or lower) and divide by 2.
    # More robust: count all pairs (i, j) with i < j and distance_matrix[i,j] <= epsilon
    count_pairs = 0
    for i in range(N_m):
        for j in range(i + 1, N_m):
            if distance_matrix[i, j] <= epsilon:
                count_pairs += 1

    # 7. Compute the correlation sum C(epsilon)
    if N_m * (N_m - 1) / 2 == 0:
        C_epsilon = 0.0
    else:
        C_epsilon = count_pairs / (N_m * (N_m - 1) / 2)

    # 8. Return log(C(epsilon)) for a given epsilon
    if C_epsilon > 0:
        return np.log(C_epsilon)
    else:
        return np.nan

print("Defined 'correlation_dimension' function.")

"""**Reasoning**:
Now that the `correlation_dimension` function is defined, I will apply it to the 'theta' signal of each utterance segment and add the calculated correlation dimension to the existing feature set for each segment. I'll use `m=2` and `epsilon=0.5` as example parameters for the `correlation_dimension` function.


"""

for i, segment_df in enumerate(utterance):
    # Extract theta signal for the current segment
    theta_signal = segment_df['theta']

    # Apply the correlation_dimension function with example parameters m=2, epsilon=0.5
    # The choice of m and epsilon often requires domain knowledge or further analysis
    correlation_dim_value = correlation_dimension(theta_signal, m=2, epsilon=0.5)

    # Update the existing feature dictionary for this segment
    current_segment_id = i + 1
    for segment_feature_dict in all_utterance_features:
        if segment_feature_dict['segment_id'] == current_segment_id:
            segment_feature_dict['theta_correlation_dimension'] = correlation_dim_value
            break

print("Correlation Dimension extracted and added to all utterance segments.")

# print the first few feature sets to verify
print("\nExample of extracted features for the first two segments with Correlation Dimension:")
print(all_utterance_features[:2])

"""## Extract Nonlinear Dynamics Features

### Subtask:
Iterate through each `segment_df` in the `utterance` list. For each segment, apply the implemented `approx_entropy`, `sample_entropy`, and `correlation_dimension` functions to the 'theta' signal. Store the calculated features for each segment, ensuring that appropriate parameters (m and r/epsilon) are chosen for each function.

**Reasoning**:
I need to extract the nonlinear dynamics features (Approximate Entropy, Sample Entropy, and Correlation Dimension) for each utterance segment's 'theta' signal. I will iterate through the segments, apply the previously defined functions with the specified parameters, and store the results in a list of dictionaries.
"""

nonlinear_features = []

for i, segment_df in enumerate(utterance):
    # Extract theta signal for the current segment
    theta_signal = segment_df['theta']

    # Calculate Approximate Entropy (ApEn)
    ap_en_value = approx_entropy(theta_signal, m=2, r=0.2)

    # Calculate Sample Entropy (SampEn)
    samp_en_value = sample_entropy(theta_signal, m=2, r=0.2)

    # Calculate Correlation Dimension
    # Note: correlation_dimension returns log(C(epsilon)), so it's already in a useful form
    corr_dim_value = correlation_dimension(theta_signal, m=2, epsilon=0.5)

    # Create a dictionary for the current segment's nonlinear features
    segment_nonlinear_features = {
        'segment_id': i + 1,
        'theta_approx_entropy': ap_en_value,
        'theta_sample_entropy': samp_en_value,
        'theta_correlation_dimension': corr_dim_value
    }

    # Append to the list
    nonlinear_features.append(segment_nonlinear_features)

print("Nonlinear dynamics features extracted for all utterance segments.")

# print the extracted features for the first few segments to verify
print("\nExample of extracted nonlinear features for the first two segments:")
print(nonlinear_features[:2])

"""## Create Nonlinear Features DataFrame

### Subtask:
Consolidate all the extracted nonlinear dynamics features (Approximate Entropy, Sample Entropy, and Correlation Dimension) for all utterance segments into a new, separate pandas DataFrame. This DataFrame should include a 'segment_id' column to identify the origin of each feature set.

**Reasoning**:
The subtask requires consolidating the extracted nonlinear dynamics features into a new pandas DataFrame. I will convert the `nonlinear_features` list of dictionaries into a DataFrame using `pd.DataFrame()` and print its head to verify.
"""

import pandas as pd

# Convert the list of nonlinear features dictionaries into a pandas DataFrame
nonlinear_features_df = pd.DataFrame(nonlinear_features)

print("Nonlinear Dynamics Features DataFrame created successfully:")
# print the first few rows of the new DataFrame
print(nonlinear_features_df.head())

"""## Final Task

### Subtask:
Review the newly created DataFrame of nonlinear dynamics features to identify significant patterns or changes in the complexity and chaotic characteristics of jaw motion across different utterance segments.

## Summary:

### Data Analysis Key Findings

*   **Function Implementations**: Three core nonlinear dynamics functions were successfully implemented:
    *   `approx_entropy`: Calculates Approximate Entropy (ApEn) with robust handling for short and constant signals, returning `np.nan` or `0.0` respectively. The tolerance `r` is scaled by the signal's standard deviation.
    *   `sample_entropy`: Calculates Sample Entropy (SampEn) with similar robust edge case handling as ApEn and `r` scaled by the signal's standard deviation. It correctly excludes self-matches in its counting process.
    *   `correlation_dimension`: Estimates the Correlation Dimension, handling short and constant signals by returning `np.nan` or `0.0`. It calculates the logarithm of the correlation sum (log(C($\epsilon$))).
*   **Feature Extraction**: Nonlinear dynamics features (Approximate Entropy, Sample Entropy, and Correlation Dimension) were successfully extracted for the 'theta' signal of all utterance segments.
    *   For ApEn and SampEn, an embedding dimension `m=2` and tolerance `r=0.2` were used.
    *   For Correlation Dimension, an embedding dimension `m=2` and radius `$\epsilon$=0.5` were used.
    *   For instance, the first segment exhibited an Approximate Entropy of approximately 10.16, Sample Entropy of 0.04, and a Correlation Dimension of -0.45.
*   **Data Consolidation**: All extracted nonlinear dynamics features were successfully consolidated into a new pandas DataFrame named `nonlinear_features_df`, which includes a `segment_id` column for easy identification of each feature set.

### Insights or Next Steps

*   The `nonlinear_features_df` now provides a quantitative basis to compare and analyze the complexity and chaotic characteristics of jaw motion (represented by the 'theta' signal) across different utterance segments, potentially revealing patterns related to speech production or articulation changes.
*   Further analysis could involve applying statistical tests or machine learning models to these features to identify significant differences between segments (e.g., different phonemes, speaking conditions) or to classify speech patterns based on their nonlinear dynamics.

# Task
Calculate pairwise Pearson correlation coefficients between 'theta', 'omega', and 'alpha' for each `segment_df` in the `utterance` list and store them.

## Calculate Pairwise Correlation Coefficients

### Subtask:
Iterate through each `segment_df` in the `utterance` list and calculate the Pearson correlation coefficients between the pairs of 'theta', 'omega', and 'alpha' columns. Store these coefficients for each segment.

**Reasoning**:
I need to initialize an empty list to store the correlation features, then iterate through each segment in the `utterance` list, calculate the Pearson correlation coefficients for the specified pairs ('theta' and 'omega', 'omega' and 'alpha', 'theta' and 'alpha'), store them in a dictionary with the segment ID, append the dictionary to the list, and finally print the results.
"""

correlation_features = []

for i, segment_df in enumerate(utterance):
    # Calculate the Pearson correlation coefficient between 'theta' and 'omega'
    theta_omega_corr = segment_df['theta'].corr(segment_df['omega'])

    # Calculate the Pearson correlation coefficient between 'omega' and 'alpha'
    omega_alpha_corr = segment_df['omega'].corr(segment_df['alpha'])

    # Calculate the Pearson correlation coefficient between 'theta' and 'alpha'
    theta_alpha_corr = segment_df['theta'].corr(segment_df['alpha'])

    # Create a dictionary for the current segment's correlation features
    segment_correlation = {
        'segment_id': i + 1,
        'theta_omega_correlation': theta_omega_corr,
        'omega_alpha_correlation': omega_alpha_corr,
        'theta_alpha_correlation': theta_alpha_corr
    }

    # Append to the list
    correlation_features.append(segment_correlation)

print("Pearson correlation coefficients calculated for all utterance segments.")

# print the extracted features for the first few segments to verify
print("\nExample of extracted correlation features for the first two segments:")
print(correlation_features[:2])

"""**Reasoning**:
The subtask requires consolidating the extracted correlation features into a new pandas DataFrame. I will convert the `correlation_features` list of dictionaries into a DataFrame and print its head to verify the results.


"""

import pandas as pd

# Convert the list of correlation features dictionaries into a pandas DataFrame
correlation_features_df = pd.DataFrame(correlation_features)

print("Correlation Features DataFrame created successfully:")
# print the first few rows of the new DataFrame
print(correlation_features_df.head())

"""## Summary:

### Data Analysis Key Findings
*   **Correlation Feature Extraction**: Pearson correlation coefficients were successfully calculated for the pairs ('theta', 'omega'), ('omega', 'alpha'), and ('theta', 'alpha') for each utterance segment. For instance, in the first segment, the `theta_omega_correlation` was approximately -0.214, `omega_alpha_correlation` was close to 0, and `theta_alpha_correlation` was approximately -0.111.
*   **Mutual Information (MI) Calculation**: A `mutual_information` function was implemented to quantify non-linear dependencies. This function handles edge cases and discretizes continuous signals for MI calculation.
*   **Mutual Information Values**: Mutual information was computed for ('theta', 'omega'), ('omega', 'alpha'), and ('theta', 'alpha') for each segment. For segment 1, `mi_theta_omega` was approximately 0.473, `mi_omega_alpha` was about 0.001, and `mi_theta_alpha` was roughly 0.024.
*   **Data Consolidation**: All extracted correlation features and mutual information values were successfully merged into a single `cross_variable_features_df` DataFrame, using `segment_id` for alignment. This consolidated DataFrame provides a comprehensive view of both linear and non-linear relationships.
*   **Phase-Plane Plot Generation**: Phase-plane plots were generated for selected utterance segments (e.g., segments 1, 2, and 3). These plots visualize 'theta' versus 'omega' and 'omega' versus 'alpha', offering qualitative insights into the dynamic patterns and trajectories of jaw motion.

### Insights or Next Steps
*   The `cross_variable_features_df` now provides a rich quantitative dataset to analyze both linear (Pearson correlation) and non-linear (mutual information) relationships between `theta`, `omega`, and `alpha` across utterance segments. This allows for a deeper understanding of the coordinated dynamics.
*   Further analysis could involve correlating these calculated features with linguistic or phonetic events to identify how these relationships change based on speech content, and performing cluster analysis on `cross_variable_features_df` to group segments with similar kinematic coupling patterns.
"""

import os

def process_recording_file(csv_path, word, subject):
    """Process a single recording CSV and return list of feature dicts (one per segment)."""
    print(f"  [START] processing file: {csv_path}")
    try:
        df = pd.read_csv(csv_path)
    except Exception as e:
        print(f"  Failed to read {csv_path}: {e}")
        return []

    # keep the top 1800 values (as in original script)
    df_new = df.head(1800)

    # calculate sampling rate (fallback to 1 if duration not available)
    total_frames = len(df_new)
    duration = 60
    sampling_rate = total_frames / duration if duration > 0 else 1.0

    # segment into 5s intervals
    frames_per_interval = int(round(5 * sampling_rate))
    if frames_per_interval <= 0:
        print(f"  Invalid frames_per_interval for {csv_path}. Skipping.")
        return []

    n_segments = total_frames // frames_per_interval
    segments = []
    for i in range(n_segments):
        start = i * frames_per_interval
        end = start + frames_per_interval
        segments.append(df_new.iloc[start:end].copy().reset_index(drop=True))

    if len(segments) < 3:
        print(f"  Not enough segments in {csv_path}. Need at least 3 (ref_min, ref_max, utterances).")
        return []

    # first two are references
    ref_min, ref_max = segments[:2]
    utterance_segments = segments[2:]

    # reference angle
    ref_min = ref_min.fillna(0.0)
    reference_angle = ref_min[ref_min.get("theta", pd.Series()) > 0]["theta"].mean() if "theta" in ref_min.columns else 0.0
    reference_max_angle = ref_max["theta"].max() if "theta" in ref_max.columns else np.nan

    # Fill NaNs in each utterance segment using KNNImputer
    print("    [STEP] NaN imputation (KNNImputer) - start")
    imputer = KNNImputer(n_neighbors=5)
    for idx, s in enumerate(utterance_segments):
        if s.isnull().any().any():
            try:
                imputed = imputer.fit_transform(s.copy())
                imputed_df = pd.DataFrame(imputed, columns=s.columns, index=s.index)
                utterance_segments[idx] = imputed_df
            except Exception as e:
                # if imputer fails, fallback to simple fill
                utterance_segments[idx] = s.fillna(method='ffill').fillna(method='bfill').fillna(0.0)
    print("    [STEP] NaN imputation (KNNImputer) - done")

    # remove spikes and interpolate
    print("    [STEP] Spike removal & interpolation - start")
    for idx, s in enumerate(utterance_segments):
        if "theta" in s.columns:
            spikes_mask = (s['theta'] < reference_angle) | (s['theta'] > reference_max_angle)
            s.loc[spikes_mask, 'theta'] = np.nan
            s['theta'] = s['theta'].interpolate(method='linear', limit_direction='both', limit_area='inside')
            utterance_segments[idx] = s
    print("    [STEP] Spike removal & interpolation - done")

    # normalize theta (z-score)
    print("    [STEP] Normalization (theta) - start")
    for idx, s in enumerate(utterance_segments):
        if "theta" in s.columns:
            t = s['theta']
            if t.std() == 0 or np.isnan(t.std()):
                s['theta'] = 0.0
            else:
                s['theta'] = (t - t.mean()) / (t.std())
            utterance_segments[idx] = s
    print("    [STEP] Normalization (theta) - done")

    # extract features for each segment
    print("    [STEP] Feature extraction - start")
    feature_list = []
    numeric_cols = ['theta', 'x', 'y', 'omega', 'alpha']

    for i, s in enumerate(utterance_segments):
        seg_id = i + 1
        seg_feat = {
            'word': word,
            'subject': subject,
            'segment_id': seg_id,
            'sampling_rate': sampling_rate,
            'frames_per_interval': frames_per_interval
        }

        # Statistical features
        for col in numeric_cols:
            if col in s.columns:
                seg_feat[f'{col}_skewness'] = s[col].skew()
                seg_feat[f'{col}_kurtosis'] = s[col].kurt()
                seg_feat[f'{col}_energy'] = np.sum(s[col].astype(float)**2)
            else:
                seg_feat[f'{col}_skewness'] = np.nan
                seg_feat[f'{col}_kurtosis'] = np.nan
                seg_feat[f'{col}_energy'] = np.nan

        # Zero Crossing Rates for omega and alpha
        def _zcr(series):
            series = series.dropna()
            if series.empty:
                return 0.0
            return np.sum(np.abs(np.diff(np.sign(series)))) / (2 * len(series))

        seg_feat['omega_zcr'] = _zcr(s['omega']) if 'omega' in s.columns else np.nan
        seg_feat['alpha_zcr'] = _zcr(s['alpha']) if 'alpha' in s.columns else np.nan

        # Spectral features for theta
        if 'theta' in s.columns:
            sf = extract_spectral_features(s['theta'], sampling_rate)
            for k, v in sf.items():
                seg_feat[f'theta_{k}'] = v
        else:
            for k in ['dominant_frequency','spectral_centroid','spectral_bandwidth','spectral_entropy','power_spectral_density']:
                seg_feat[f'theta_{k}'] = np.nan

        # Nonlinear features
        if 'theta' in s.columns:
            seg_feat['theta_approx_entropy'] = approx_entropy(s['theta'], m=2, r=0.2)
            seg_feat['theta_sample_entropy'] = sample_entropy(s['theta'], m=2, r=0.2)
            seg_feat['theta_correlation_dimension'] = correlation_dimension(s['theta'], m=2, epsilon=0.5)
        else:
            seg_feat['theta_approx_entropy'] = np.nan
            seg_feat['theta_sample_entropy'] = np.nan
            seg_feat['theta_correlation_dimension'] = np.nan

        # Pairwise Pearson correlations
        def _corr(a, b):
            if a in s.columns and b in s.columns:
                return s[a].corr(s[b])
            return np.nan

        seg_feat['theta_omega_correlation'] = _corr('theta', 'omega')
        seg_feat['omega_alpha_correlation'] = _corr('omega', 'alpha')
        seg_feat['theta_alpha_correlation'] = _corr('theta', 'alpha')

        # Mutual information (theta-omega, omega-alpha, theta-alpha)
        def _mi(a, b):
            if a in s.columns and b in s.columns:
                return mutual_information(s[a], s[b], bins=10)
            return np.nan

        seg_feat['theta_omega_mi'] = _mi('theta','omega')
        seg_feat['omega_alpha_mi'] = _mi('omega','alpha')
        seg_feat['theta_alpha_mi'] = _mi('theta','alpha')

        feature_list.append(seg_feat)

    print(f"    [STEP] Feature extraction - done ({len(feature_list)} segments)")
    print(f"  [END] processing file: {csv_path}")
    return feature_list

# --- Added: mutual_information helper (used by feature extraction) ---
import numpy as np

def mutual_information(x, y, bins=10):
    """Estimate mutual information between two continuous variables via 2D histogram."""
    x = np.asarray(x).flatten()
    y = np.asarray(y).flatten()
    mask = ~np.isnan(x) & ~np.isnan(y)
    x, y = x[mask], y[mask]
    if x.size == 0 or y.size == 0:
        return np.nan
    c_xy, _, _ = np.histogram2d(x, y, bins=bins)
    pxy = c_xy.astype(float) / np.sum(c_xy)
    px = pxy.sum(axis=1)
    py = pxy.sum(axis=0)
    mi = 0.0
    for i in range(pxy.shape[0]):
        for j in range(pxy.shape[1]):
            if pxy[i, j] > 0 and px[i] > 0 and py[j] > 0:
                mi += pxy[i, j] * np.log(pxy[i, j] / (px[i] * py[j]))
    return mi

# Main loop: iterate over all words and subjects
combined_features = []
data_root = os.path.join(os.path.dirname(__file__), "Data", "recordings")

print("Beginning batch processing of all words and subjects...")
for word in WORDS:
    word_dir = os.path.join(data_root, word)
    if not os.path.isdir(word_dir):
        print(f"Skipping word '{word}': directory not found at {word_dir}")
        continue

    for subject in SUBJECTS:
        csv_name = f"{subject}.csv"
        csv_path = os.path.join(word_dir, csv_name)
        if not os.path.isfile(csv_path):
            print(f"  Missing file for {word}/{subject}: {csv_path}. Skipping.")
            continue

        print(f"Processing {word} / {subject} ...")
        feats = process_recording_file(csv_path, word, subject)
        combined_features.extend(feats)
        print(f"Finished {word} / {subject} -> added {len(feats)} segments")

# Build combined dataframe
combined_features_df = pd.DataFrame(combined_features)
output_csv = os.path.join(os.path.dirname(__file__), "Data", "combined_features.csv")
try:
    combined_features_df.to_csv(output_csv, index=False)
    print(f"Combined features saved to {output_csv}")
except Exception as e:
    print(f"Failed to save combined features CSV: {e}")

print("Batch processing complete. Combined DataFrame ready as 'combined_features_df'.")

# --- Added: Move all plotting to the end of the script (single location) ---
# (This block intentionally placed at the end so plots are created after all processing)
import matplotlib.pyplot as plt

if 'combined_features_df' in globals() and not combined_features_df.empty:
    print("[PLOTS] Generating summary plots - start")

    # Example: number of segments per word
    plt.figure(figsize=(12, 4))
    combined_features_df.groupby('word')['segment_id'].count().sort_values().plot(kind='barh')
    plt.title('Number of extracted segments per word')
    plt.xlabel('Number of segments')
    plt.tight_layout()
    plt.show()

    # Spectral features overview (if present)
    spec_cols = [c for c in combined_features_df.columns if c.startswith('theta_') and c not in ('theta_approx_entropy','theta_sample_entropy','theta_correlation_dimension')]
    if spec_cols:
        fig, axes = plt.subplots(len(spec_cols), 1, figsize=(12, 4 * len(spec_cols)), sharex=True)
        for i, feat in enumerate(spec_cols):
            axes[i].plot(combined_features_df['segment_id'], combined_features_df[feat], marker='o')
            axes[i].set_title(feat)
            axes[i].grid(True)
        axes[-1].set_xlabel('Segment ID')
        plt.tight_layout()
        plt.show()

    print("[PLOTS] Generating summary plots - done")
else:
    print("[PLOTS] No combined_features_df available or empty - skipping plots")

# ...existing code...

